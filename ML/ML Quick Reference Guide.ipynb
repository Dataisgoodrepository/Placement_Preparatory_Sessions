{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d13bdc3",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "## I. Data Preparation for Modelling\n",
    "\n",
    "1. [Types of Encoding](#types-of-encoding)\n",
    "2. [Data Transformation Techniques](#data-transformation-techniques)\n",
    "3. [Principal Component Analysis](#principal-component-analysis)\n",
    "4. [Imbalance Treatment Methods](#imbalance-treatment-methods)\n",
    "5. [Train Test Split](#train-test-split)\n",
    "\n",
    "## II. Model Building\n",
    "\n",
    "1. [Regression Algorithm (OLS)](#regression-algorithm-ols)\n",
    "2. [Classification Algorithms](#classification-algorithms)\n",
    "    - [Logistic Regression](#logistic-regression)\n",
    "    - [SVM](#svm)\n",
    "    - [KNN](#knn)\n",
    "    - [Naive Bayes](#naive-bayes)\n",
    "    - [Decision Tree](#decision-tree)\n",
    "3. [Clustering Algorithms](#clustering-algorithms)\n",
    "    - [K-Means](#k-means)\n",
    "    - [Hierarchical](#hierarchical)\n",
    "    - [DBScan](#dbscan)\n",
    "\n",
    "## III. Ensembling Techniques\n",
    "\n",
    "1. [Bagging (Random Forest)](#bagging-random-forest)\n",
    "2. [Boosting](#boosting)\n",
    "    - [AdaBoost](#adaboost)\n",
    "    - [GradBoost](#gradboost)\n",
    "    - [XGBoost](#xgboost)\n",
    "\n",
    "## IV. Model Evaluation\n",
    "\n",
    "1. [Confusion Matrix](#confusion-matrix)\n",
    "2. [Accuracy Metrics](#accuracy-metrics)\n",
    "3. [Model Validation Techniques](#model-validation-techniques)\n",
    "    - [k-Fold Cross Validation](#k-fold-cross-validation)\n",
    "    - [LOOCV](#loocv)\n",
    "\n",
    "## V. Regularization\n",
    "\n",
    "1. [Ridge](#Ridge)\n",
    "2. [Lasso](#Lasso)\n",
    "3. [ElasticNet](#ElasticNet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee86a76",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba7b06",
   "metadata": {},
   "source": [
    "# I. Data Preparation for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f4b6c6",
   "metadata": {},
   "source": [
    "## <a id=\"types-of-encoding\"></a>Types of Encoding\n",
    "\n",
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc13b9c",
   "metadata": {},
   "source": [
    "Converts categorical values into integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f7641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['category'] = le.fit_transform(df['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406cf94b",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6105502",
   "metadata": {},
   "source": [
    "Creates binary columns for each category in a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386de6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "encoded_features = ohe.fit_transform(df[['category']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbda7f6",
   "metadata": {},
   "source": [
    "#### Binary Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878ee1a5",
   "metadata": {},
   "source": [
    "Converts categories into binary code and then splits the digits into separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a02e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "be = ce.BinaryEncoder()\n",
    "df = be.fit_transform(df['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fee424",
   "metadata": {},
   "source": [
    "#### Target Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4341b736",
   "metadata": {},
   "source": [
    "Replaces each category with the mean of the target variable for that category. \n",
    "Useful in situations where there is a strong relationship between categorical features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34413f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "te = ce.TargetEncoder()\n",
    "df['category'] = te.fit_transform(df['category'], df['target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304f780",
   "metadata": {},
   "source": [
    "#### Frequency Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a92146",
   "metadata": {},
   "source": [
    "Replaces each category with its frequency in the dataset.\n",
    "Useful when the frequency of categories is significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b14ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_freq'] = df['category'].map(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad8fe0",
   "metadata": {},
   "source": [
    "#### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf4ff85",
   "metadata": {},
   "source": [
    "Converts categories into integers but respects the order of the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "oe = OrdinalEncoder()\n",
    "df['category'] = oe.fit_transform(df[['category']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe694b7",
   "metadata": {},
   "source": [
    "## <a id=\"data-transformation-techniques\"></a>Data Transformation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ee1a0d",
   "metadata": {},
   "source": [
    "#### Normalization (Min-Max Scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d6c26",
   "metadata": {},
   "source": [
    "Rescales the data to fit within a specific range, typically [0, 1].\n",
    "\n",
    "Useful when the data needs to be brought to a common scale without distorting differences in the ranges of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c201b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e86790",
   "metadata": {},
   "source": [
    "#### Standardization (Z-score Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff9d92f",
   "metadata": {},
   "source": [
    "Rescales data to have a mean of 0 and a standard deviation of 1.\n",
    "Usage: Useful when the data has different units or scales, especially for algorithms that assume normally distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220b98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_standardized = scaler.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d842804",
   "metadata": {},
   "source": [
    "#### Log Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a900040",
   "metadata": {},
   "source": [
    "Applies the logarithm function to the data.\n",
    "\n",
    "Useful for reducing skewness and handling exponential growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad4e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['log_transformed'] = np.log(df['column'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85bf1d3",
   "metadata": {},
   "source": [
    "#### Square Root Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c85e6e",
   "metadata": {},
   "source": [
    "Applies the square root function to the data.\n",
    "\n",
    "Useful for stabilizing variance and making the data more normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1e32be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sqrt_transformed'] = np.sqrt(df['column'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932debf7",
   "metadata": {},
   "source": [
    "#### Power Transformation (Box-Cox Transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae7f206",
   "metadata": {},
   "source": [
    "Applies a power transformation to stabilize variance and make the data more normally distributed.\n",
    "\n",
    "Useful for dealing with skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db44cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "df['boxcox_transformed'], _ = boxcox(df['column'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb453f",
   "metadata": {},
   "source": [
    "#### Exponential Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e699c3f",
   "metadata": {},
   "source": [
    "Applies the exponential function to the data.\n",
    "\n",
    "Can be used to reverse log transformations or to handle data that grows exponentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5167f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['exp_transformed'] = np.exp(df['column'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f06b693",
   "metadata": {},
   "source": [
    "#### Quantile Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f7a70e",
   "metadata": {},
   "source": [
    "Transforms features to follow a uniform or normal distribution.\n",
    "\n",
    "Useful for making non-normal data more normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb97f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "qt = QuantileTransformer(output_distribution='normal')\n",
    "df_quantile_transformed = qt.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216905c3",
   "metadata": {},
   "source": [
    "#### Rank Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2146d6",
   "metadata": {},
   "source": [
    "Replaces values with their rank in the data.\n",
    "\n",
    "Useful for non-parametric methods and handling ordinal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e35747",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rank_transformed'] = df['column'].rank()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f1afb9",
   "metadata": {},
   "source": [
    "#### Binning (Discretization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37235e2e",
   "metadata": {},
   "source": [
    "Converts continuous data into discrete bins.\n",
    "\n",
    "Useful for reducing the effect of minor observation errors and managing outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe58d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['binned'] = pd.cut(df['column'], bins=3, labels=[\"low\", \"medium\", \"high\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f01e065",
   "metadata": {},
   "source": [
    "#### Polynomial Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea98db",
   "metadata": {},
   "source": [
    "Generates polynomial and interaction features.\n",
    "\n",
    "Useful for modeling non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "df_poly = poly.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a1f1db",
   "metadata": {},
   "source": [
    "#### Box-Cox Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320bf5c3",
   "metadata": {},
   "source": [
    "A type of power transformation that stabilizes variance and makes the data more normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17152a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "df['boxcox_transformed'], _ = boxcox(df['column'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6274da9",
   "metadata": {},
   "source": [
    "#### Yeojohnson Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb5704f",
   "metadata": {},
   "source": [
    "Definition: A power transformation technique that works on both positive and negative values.\n",
    "\n",
    "Usage: Useful for achieving normality in skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2931ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "df_yeojohnson_transformed = pt.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3a53e",
   "metadata": {},
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a6a5e",
   "metadata": {},
   "source": [
    "Adjusts the scale of the features to bring them into alignment.\n",
    "\n",
    "Ensures features contribute equally to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ac84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3611e952",
   "metadata": {},
   "source": [
    "#### Logistic Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e03810",
   "metadata": {},
   "source": [
    "Transforms data using the logistic function.\n",
    "\n",
    "Useful for transforming binary outcomes or probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['logistic_transformed'] = 1 / (1 + np.exp(-df['column']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd09d3",
   "metadata": {},
   "source": [
    "## <a id=\"principal-component-analysis\"></a>Principal Component Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc73935",
   "metadata": {},
   "source": [
    "A statistical procedure that uses an orthogonal transformation to convert a set of possibly correlated variables into a set of linearly uncorrelated variables called principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed1e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_standardized = scaler.fit_transform(df)\n",
    "\n",
    "#Compute the Covariance Matrix\n",
    "import numpy as np\n",
    "covariance_matrix = np.cov(df_standardized.T)\n",
    "\n",
    "#Calculate Eigenvalues and Eigenvectors\n",
    "eig_vals, eig_vecs = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "#Sort Eigenvalues and Eigenvectors\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "#Form the Principal Components\n",
    "num_components = 2  # or any other number of desired components\n",
    "selected_vectors = np.hstack([eig_pairs[i][1].reshape(df_standardized.shape[1], 1) for i in range(num_components)])\n",
    "\n",
    "#Transform the Original Dataset\n",
    "df_pca = df_standardized.dot(selected_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33fa63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using a library\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "df_pca = pca.fit_transform(df_standardized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5377dbdd",
   "metadata": {},
   "source": [
    "#### Explained Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be88e2e1",
   "metadata": {},
   "source": [
    "The proportion of the datasets variance that each principal component explains.\n",
    "\n",
    "Helps determine the number of principal components to retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa45aa19",
   "metadata": {},
   "source": [
    "## <a id=\"imbalance-treatment-methods\"></a>Imbalance Treatment Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a04beb",
   "metadata": {},
   "source": [
    "#### Oversampling\n",
    "\n",
    "Using SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f2a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Generate a sample dataset with class imbalance\n",
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "                           weights=[0.1, 0.9], n_informative=3, n_redundant=1,\n",
    "                           flip_y=0, n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=1000, random_state=42)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Print the class distribution before applying SMOTE\n",
    "print(\"Class distribution before SMOTE:\", Counter(y_train))\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Fit and transform the training data using SMOTE\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print the class distribution after applying SMOTE\n",
    "print(\"Class distribution after SMOTE:\", Counter(y_resampled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949e466",
   "metadata": {},
   "source": [
    "#### Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce7b7e5",
   "metadata": {},
   "source": [
    "Reducing the number of majority class samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84165ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "undersample = RandomUnderSampler()\n",
    "X_res, y_res = undersample.fit_resample(X, y)\n",
    "\n",
    "# X: features from your dataset, an array\n",
    "# y: labels in dataset, an array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c5862",
   "metadata": {},
   "source": [
    "# II. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f145c",
   "metadata": {},
   "source": [
    "## <a id=\"regression-algorithm-ols\"></a>Regression Algorithm (OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0020f9",
   "metadata": {},
   "source": [
    "Linear regression is used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de74669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaefc74c",
   "metadata": {},
   "source": [
    "## <a id=\"classification-algorithms\"></a>Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a859ea",
   "metadata": {},
   "source": [
    "### <a id=\"logistic-regression\"></a>Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f65d51",
   "metadata": {},
   "source": [
    "Logistic regression is used for binary classification problems. It models the probability that a given input belongs to a particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e22dcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assuming binary target variable\n",
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# max_iter: maximumm number of iteration you want your algorithm to perform\n",
    "# Default Value: max_iter defaults to 100.\n",
    "# If the algorithm does not converge within the default number of iterations, you may receive a ConvergenceWarning. \n",
    "# In such cases, you should increase max_iter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b9054",
   "metadata": {},
   "source": [
    "### <a id=\"svm\"></a>SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16865a03",
   "metadata": {},
   "source": [
    "SVM is used for classification and regression tasks. \n",
    "\n",
    "It finds the hyperplane that best separates the classes in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0598f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7926de2",
   "metadata": {},
   "source": [
    "### <a id=\"knn\"></a>KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a898c",
   "metadata": {},
   "source": [
    "KNN is a simple, non-parametric algorithm used for both classification and regression tasks. \n",
    "\n",
    "It classifies a data point based on the majority class among its k-nearest neighbors in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36125115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "#n_neighbours: numeber of neighbours you want to consider for KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5598d0",
   "metadata": {},
   "source": [
    "### <a id=\"naive-bayes\"></a>Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c40ea1",
   "metadata": {},
   "source": [
    "Naive Bayes is a probabilistic classifier based on Bayes theorem. \n",
    "\n",
    "It assumes independence between predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679d13bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b93cf",
   "metadata": {},
   "source": [
    "### <a id=\"decision-tree\"></a>Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52785a86",
   "metadata": {},
   "source": [
    "Decision trees are used for both classification and regression tasks. \n",
    "\n",
    "They split the data into subsets based on the value of input features, creating a tree-like model of decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afbebcd",
   "metadata": {},
   "source": [
    "## <a id=\"clustering-algorithms\"></a>Clustering Algorithms\n",
    "\n",
    "### <a id=\"k-means\"></a>K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c77695",
   "metadata": {},
   "source": [
    "Partitions data into k clusters, where each data point belongs to the cluster with the nearest mean.\n",
    "\n",
    "Suitable for large datasets where cluster boundaries are spherical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c618049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "labels = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41763c",
   "metadata": {},
   "source": [
    "### <a id=\"hierarchical\"></a>Hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94b0e9",
   "metadata": {},
   "source": [
    "Builds a hierarchy of clusters using either agglomerative (bottom-up) or divisive (top-down) approaches.\n",
    "\n",
    "Useful for creating dendrograms to visualize the data’s nested structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f9370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "linked = linkage(X, method='ward')\n",
    "dendrogram(linked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e8e49",
   "metadata": {},
   "source": [
    "### <a id=\"dbscan\"></a>DBScan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb68ea",
   "metadata": {},
   "source": [
    "Groups points that are closely packed together and marks points that lie alone in low-density regions as outliers.\n",
    "\n",
    "Effective for datasets with noise and varying density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bc4fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4473d0da",
   "metadata": {},
   "source": [
    "# III. Ensembling Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e845e8f",
   "metadata": {},
   "source": [
    "## <a id=\"bagging-random-forest\"></a>Bagging (Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca3f1e7",
   "metadata": {},
   "source": [
    "An extension of bagging that constructs multiple decision trees and merges them to get a more accurate and stable prediction.\n",
    "\n",
    "Handles both classification and regression tasks well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33102a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734b8744",
   "metadata": {},
   "source": [
    "## <a id=\"boosting\"></a>Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f4af0",
   "metadata": {},
   "source": [
    "Combines multiple models (usually of the same type) to reduce variance and improve stability.\n",
    "\n",
    "Useful for algorithms that exhibit high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a2d5f",
   "metadata": {},
   "source": [
    "### <a id=\"adaboost\"></a>AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe666d7c",
   "metadata": {},
   "source": [
    " A type of boosting that adjusts the weights of incorrectly classified instances, focusing more on difficult cases.\n",
    "\n",
    "Useful for improving the performance of weak classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7701af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(n_estimators=50)\n",
    "ada.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a9a5ee",
   "metadata": {},
   "source": [
    "### <a id=\"gradboost\"></a>GradBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7833830b",
   "metadata": {},
   "source": [
    "Sequentially builds models that minimize a loss function, typically using decision trees.\n",
    "\n",
    "Suitable for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d61898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=100)\n",
    "gbc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cacefd",
   "metadata": {},
   "source": [
    "### <a id=\"xgboost\"></a>XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff61fe4a",
   "metadata": {},
   "source": [
    "An optimized implementation of gradient boosting designed for speed and performance.\n",
    "\n",
    "Highly efficient and scalable for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384023a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100)\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56fd04b",
   "metadata": {},
   "source": [
    "# IV. Model Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d91f5",
   "metadata": {},
   "source": [
    "## <a id=\"confusion-matrix\"></a>Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e60877",
   "metadata": {},
   "source": [
    "A table that summarizes the performance of a classification model by showing the true vs. predicted classifications.\n",
    "\n",
    "Useful for evaluating the accuracy of a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b7b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382b949",
   "metadata": {},
   "source": [
    "## <a id=\"accuracy-metrics\"></a>Accuracy Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeb3c6d",
   "metadata": {},
   "source": [
    "Metrics used to evaluate the performance of a model, including accuracy, precision, recall, F1 score, etc.\n",
    "\n",
    "Provides different perspectives on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d3c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c8a33",
   "metadata": {},
   "source": [
    "## <a id=\"model-validation-techniques\"></a>Model Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd3b3c",
   "metadata": {},
   "source": [
    "### <a id=\"k-fold-cross-validation\"></a>k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5555e4aa",
   "metadata": {},
   "source": [
    "Splits the data into k subsets and trains the model k times, each time using a different subset as the validation set and the remaining as the training set.\n",
    "\n",
    "Provides a more reliable estimate of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e7e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "kf = KFold(n_splits=5)\n",
    "scores = cross_val_score(model, X, y, cv=kf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f58b4",
   "metadata": {},
   "source": [
    "### <a id=\"loocv\"></a>LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e19199",
   "metadata": {},
   "source": [
    "A special case of k-fold cross-validation where k equals the number of observations, i.e., each instance is used once as a validation set.\n",
    "\n",
    "Useful for small datasets to maximize training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "scores = cross_val_score(model, X, y, cv=loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f4c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f669cc9",
   "metadata": {},
   "source": [
    "# V. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803369dc",
   "metadata": {},
   "source": [
    "## <a id=\"Ridge\"></a>Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02313d5",
   "metadata": {},
   "source": [
    "Adds a penalty equal to the sum of the squared coefficients to the loss function, discouraging large coefficients.\n",
    "\n",
    "Reduces overfitting by constraining the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ed6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796443de",
   "metadata": {},
   "source": [
    "## <a id=\"Lasso\"></a>Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4947715b",
   "metadata": {},
   "source": [
    "Adds a penalty equal to the absolute value of the coefficients to the loss function, leading to sparse models.\n",
    "\n",
    "Useful for feature selection by shrinking some coefficients to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080067d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d512d",
   "metadata": {},
   "source": [
    "## <a id=\"ElasticNet\"></a>ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2e4b8f",
   "metadata": {},
   "source": [
    "Combines L1 and L2 regularization, adding both penalties to the loss function.\n",
    "\n",
    "Useful when there are multiple features with high collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "elastic_net.fit(X_train, y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
