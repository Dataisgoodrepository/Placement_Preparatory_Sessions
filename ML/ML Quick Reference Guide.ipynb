{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d13bdc3",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "## I. Data Preparation for Modelling\n",
    "\n",
    "1. [Types of Encoding](#types-of-encoding)\n",
    "2. [Data Transformation Techniques](#data-transformation-techniques)\n",
    "3. [Principal Component Analysis](#principal-component-analysis)\n",
    "4. [Imbalance Treatment Methods](#imbalance-treatment-methods)\n",
    "5. [Train Test Split](#train-test-split)\n",
    "\n",
    "## II. Model Building\n",
    "\n",
    "1. [Regression Algorithm (OLS)](#regression-algorithm-ols)\n",
    "2. [Classification Algorithms](#classification-algorithms)\n",
    "    - [Logistic Regression](#logistic-regression)\n",
    "    - [SVM](#svm)\n",
    "    - [KNN](#knn)\n",
    "    - [Naive Bayes](#naive-bayes)\n",
    "    - [Decision Tree](#decision-tree)\n",
    "3. [Clustering Algorithms](#clustering-algorithms)\n",
    "    - [K-Means](#k-means)\n",
    "    - [Hierarchical](#hierarchical)\n",
    "    - [DBScan](#dbscan)\n",
    "\n",
    "## III. Ensembling Techniques\n",
    "\n",
    "1. [Bagging (Random Forest)](#bagging-random-forest)\n",
    "2. [Boosting](#boosting)\n",
    "    - [AdaBoost](#adaboost)\n",
    "    - [GradBoost](#gradboost)\n",
    "    - [XGBoost](#xgboost)\n",
    "\n",
    "## IV. Model Evaluation\n",
    "\n",
    "1. [Confusion Matrix](#confusion-matrix)\n",
    "2. [Accuracy Metrics](#accuracy-metrics)\n",
    "3. [Model Validation Techniques](#model-validation-techniques)\n",
    "    - [k-Fold Cross Validation](#k-fold-cross-validation)\n",
    "    - [LOOCV](#loocv)\n",
    "\n",
    "## V. Regularization\n",
    "\n",
    "1. [Ridge](#Ridge)\n",
    "2. [Lasso](#Lasso)\n",
    "3. [ElasticNet](#ElasticNet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee86a76",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba7b06",
   "metadata": {},
   "source": [
    "# I. Data Preparation for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f4b6c6",
   "metadata": {},
   "source": [
    "## <a id=\"types-of-encoding\"></a>Types of Encoding\n",
    "\n",
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc13b9c",
   "metadata": {},
   "source": [
    "Converts categorical values into integer values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4a9927",
   "metadata": {},
   "source": [
    "Assigns a unique integer to each category. Example for \"Color\": Red → 0, Green → 1, Blue → 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f7641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['category'] = le.fit_transform(df['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406cf94b",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6105502",
   "metadata": {},
   "source": [
    "Creates binary columns for each category in a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7915da",
   "metadata": {},
   "source": [
    "Example: For a categorical feature \"Color\" with values \"Red,\" \"Green,\" and \"Blue,\" one-hot encoding creates three new columns: \"Color_Red,\" \"Color_Green,\" and \"Color_Blue.\" If a sample is \"Green,\" the encoded vector would be [0, 1, 0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386de6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "encoded_features = ohe.fit_transform(df[['category']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbda7f6",
   "metadata": {},
   "source": [
    "#### Binary Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878ee1a5",
   "metadata": {},
   "source": [
    "Converts categories into binary code and then splits the digits into separate columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9674316e",
   "metadata": {},
   "source": [
    "Example: For the same \"Color\" feature with values \"Red,\" \"Green,\" and \"Blue,\" binary encoding might map \"Red\" to 01, \"Green\" to 10, and \"Blue\" to 11. This would result in 2 columns for binary representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a02e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "be = ce.BinaryEncoder()\n",
    "df = be.fit_transform(df['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fee424",
   "metadata": {},
   "source": [
    "#### Target Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4341b736",
   "metadata": {},
   "source": [
    "Replaces each category with the mean of the target variable for that category. \n",
    "\n",
    "Useful in situations where there is a strong relationship between categorical features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34413f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "te = ce.TargetEncoder()\n",
    "df['category'] = te.fit_transform(df['category'], df['target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304f780",
   "metadata": {},
   "source": [
    "#### Frequency Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a92146",
   "metadata": {},
   "source": [
    "Replaces each category with its frequency in the dataset.\n",
    "\n",
    "Useful when the frequency of categories is significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b14ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_freq'] = df['category'].map(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad8fe0",
   "metadata": {},
   "source": [
    "#### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf4ff85",
   "metadata": {},
   "source": [
    "Converts categories into integers but respects the order of the categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6531fe90",
   "metadata": {},
   "source": [
    "Assigns integers based on an inherent order. Example: Small → 1, Medium → 2, Large → 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "oe = OrdinalEncoder()\n",
    "df['category'] = oe.fit_transform(df[['category']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe694b7",
   "metadata": {},
   "source": [
    "## <a id=\"data-transformation-techniques\"></a>Data Transformation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ee1a0d",
   "metadata": {},
   "source": [
    "#### Normalization (Min-Max Scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d6c26",
   "metadata": {},
   "source": [
    "Rescales the data to fit within a specific range, typically [0, 1].\n",
    "\n",
    "Useful when the data needs to be brought to a common scale without distorting differences in the ranges of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c201b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e86790",
   "metadata": {},
   "source": [
    "#### Standardization (Z-score Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff9d92f",
   "metadata": {},
   "source": [
    "Rescales data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Useful when the data has different units or scales, especially for algorithms that assume normally distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220b98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_standardized = scaler.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d842804",
   "metadata": {},
   "source": [
    "#### Log Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a900040",
   "metadata": {},
   "source": [
    "Applies the logarithm function to the data.\n",
    "\n",
    "Useful for reducing skewness and handling exponential growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad4e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['log_transformed'] = np.log(df['column'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85bf1d3",
   "metadata": {},
   "source": [
    "#### Square Root Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c85e6e",
   "metadata": {},
   "source": [
    "Applies the square root function to the data.\n",
    "\n",
    "Useful for stabilizing variance and making the data more normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1e32be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sqrt_transformed'] = np.sqrt(df['column'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932debf7",
   "metadata": {},
   "source": [
    "#### Power Transformation (Box-Cox Transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae7f206",
   "metadata": {},
   "source": [
    "Applies a power transformation to stabilize variance and make the data more normally distributed.\n",
    "\n",
    "Useful for dealing with skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db44cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "df['boxcox_transformed'], _ = boxcox(df['column'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb453f",
   "metadata": {},
   "source": [
    "#### Exponential Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e699c3f",
   "metadata": {},
   "source": [
    "Applies the exponential function to the data.\n",
    "\n",
    "Can be used to reverse log transformations or to handle data that grows exponentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5167f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['exp_transformed'] = np.exp(df['column'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f06b693",
   "metadata": {},
   "source": [
    "#### Quantile Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f7a70e",
   "metadata": {},
   "source": [
    "Transforms features to follow a uniform or normal distribution.\n",
    "\n",
    "Useful for making non-normal data more normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb97f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "qt = QuantileTransformer(output_distribution='normal')\n",
    "df_quantile_transformed = qt.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216905c3",
   "metadata": {},
   "source": [
    "#### Rank Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2146d6",
   "metadata": {},
   "source": [
    "Replaces values with their rank in the data.\n",
    "\n",
    "Useful for non-parametric methods and handling ordinal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e35747",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rank_transformed'] = df['column'].rank()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f1afb9",
   "metadata": {},
   "source": [
    "#### Binning (Discretization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37235e2e",
   "metadata": {},
   "source": [
    "Converts continuous data into discrete bins.\n",
    "\n",
    "Useful for reducing the effect of minor observation errors and managing outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe58d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['binned'] = pd.cut(df['column'], bins=3, labels=[\"low\", \"medium\", \"high\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f01e065",
   "metadata": {},
   "source": [
    "#### Polynomial Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea98db",
   "metadata": {},
   "source": [
    "Generates polynomial and interaction features.\n",
    "\n",
    "Useful for modeling non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "df_poly = poly.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6274da9",
   "metadata": {},
   "source": [
    "#### Yeojohnson Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb5704f",
   "metadata": {},
   "source": [
    "Definition: A power transformation technique that works on both positive and negative values.\n",
    "\n",
    "Usage: Useful for achieving normality in skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2931ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "df_yeojohnson_transformed = pt.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3a53e",
   "metadata": {},
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a6a5e",
   "metadata": {},
   "source": [
    "Adjusts the scale of the features to bring them into alignment.\n",
    "\n",
    "Ensures features contribute equally to the model. It uses the median and the interquartile range (IQR) to scale the data, making it more resistant to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ac84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3611e952",
   "metadata": {},
   "source": [
    "#### Logistic Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e03810",
   "metadata": {},
   "source": [
    "Transforms data using the logistic function.\n",
    "\n",
    "Useful for transforming binary outcomes or probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['logistic_transformed'] = 1 / (1 + np.exp(-df['column']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd09d3",
   "metadata": {},
   "source": [
    "## <a id=\"principal-component-analysis\"></a>Principal Component Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc73935",
   "metadata": {},
   "source": [
    "A statistical procedure that uses an orthogonal transformation to convert a set of possibly correlated variables into a set of linearly uncorrelated variables called principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed1e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_standardized = scaler.fit_transform(df)\n",
    "\n",
    "#Compute the Covariance Matrix\n",
    "import numpy as np\n",
    "covariance_matrix = np.cov(df_standardized.T)\n",
    "\n",
    "#Calculate Eigenvalues and Eigenvectors\n",
    "eig_vals, eig_vecs = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "#Sort Eigenvalues and Eigenvectors\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "#Form the Principal Components\n",
    "num_components = 2  # or any other number of desired components\n",
    "selected_vectors = np.hstack([eig_pairs[i][1].reshape(df_standardized.shape[1], 1) for i in range(num_components)])\n",
    "\n",
    "#Transform the Original Dataset\n",
    "df_pca = df_standardized.dot(selected_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33fa63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using a library\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "df_pca = pca.fit_transform(df_standardized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5377dbdd",
   "metadata": {},
   "source": [
    "#### Explained Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be88e2e1",
   "metadata": {},
   "source": [
    "The proportion of the datasets variance that each principal component explains.\n",
    "\n",
    "Helps determine the number of principal components to retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa45aa19",
   "metadata": {},
   "source": [
    "## <a id=\"imbalance-treatment-methods\"></a>Imbalance Treatment Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a04beb",
   "metadata": {},
   "source": [
    "#### Oversampling\n",
    "\n",
    "Using SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f2a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Generate a sample dataset with class imbalance\n",
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "                           weights=[0.1, 0.9], n_informative=3, n_redundant=1,\n",
    "                           flip_y=0, n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=1000, random_state=42)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Print the class distribution before applying SMOTE\n",
    "print(\"Class distribution before SMOTE:\", Counter(y_train))\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Fit and transform the training data using SMOTE\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print the class distribution after applying SMOTE\n",
    "print(\"Class distribution after SMOTE:\", Counter(y_resampled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949e466",
   "metadata": {},
   "source": [
    "#### Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce7b7e5",
   "metadata": {},
   "source": [
    "Reducing the number of majority class samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84165ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "undersample = RandomUnderSampler()\n",
    "X_res, y_res = undersample.fit_resample(X, y)\n",
    "\n",
    "# X: features from your dataset, an array\n",
    "# y: labels in dataset, an array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c5862",
   "metadata": {},
   "source": [
    "# II. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f145c",
   "metadata": {},
   "source": [
    "## <a id=\"regression-algorithm-ols\"></a>Regression Algorithm (OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0020f9",
   "metadata": {},
   "source": [
    "Linear regression is used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de74669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaefc74c",
   "metadata": {},
   "source": [
    "## <a id=\"classification-algorithms\"></a>Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a859ea",
   "metadata": {},
   "source": [
    "### <a id=\"logistic-regression\"></a>Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f65d51",
   "metadata": {},
   "source": [
    "Logistic regression is used for binary classification problems. It models the probability that a given input belongs to a particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e22dcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assuming binary target variable\n",
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# max_iter: maximumm number of iteration you want your algorithm to perform\n",
    "# Default Value: max_iter defaults to 100.\n",
    "# If the algorithm does not converge within the default number of iterations, you may receive a ConvergenceWarning. \n",
    "# In such cases, you should increase max_iter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b9054",
   "metadata": {},
   "source": [
    "### <a id=\"svm\"></a>SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16865a03",
   "metadata": {},
   "source": [
    "SVM is used for classification and regression tasks. \n",
    "\n",
    "It finds the hyperplane that best separates the classes in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0598f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7926de2",
   "metadata": {},
   "source": [
    "### <a id=\"knn\"></a>KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a898c",
   "metadata": {},
   "source": [
    "KNN is a simple, non-parametric algorithm used for both classification and regression tasks. \n",
    "\n",
    "It classifies a data point based on the majority class among its k-nearest neighbors in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36125115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "#n_neighbours: numeber of neighbours you want to consider for KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5598d0",
   "metadata": {},
   "source": [
    "### <a id=\"naive-bayes\"></a>Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c40ea1",
   "metadata": {},
   "source": [
    "Naive Bayes is a probabilistic classifier based on Bayes theorem. \n",
    "\n",
    "It assumes independence between predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679d13bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b93cf",
   "metadata": {},
   "source": [
    "### <a id=\"decision-tree\"></a>Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52785a86",
   "metadata": {},
   "source": [
    "Decision trees are used for both classification and regression tasks. \n",
    "\n",
    "They split the data into subsets based on the value of input features, creating a tree-like model of decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afbebcd",
   "metadata": {},
   "source": [
    "## <a id=\"clustering-algorithms\"></a>Clustering Algorithms\n",
    "\n",
    "### <a id=\"k-means\"></a>K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c77695",
   "metadata": {},
   "source": [
    "Partitions data into k clusters, where each data point belongs to the cluster with the nearest mean.\n",
    "\n",
    "Suitable for large datasets where cluster boundaries are spherical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c618049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "labels = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41763c",
   "metadata": {},
   "source": [
    "### <a id=\"hierarchical\"></a>Hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94b0e9",
   "metadata": {},
   "source": [
    "Builds a hierarchy of clusters using either agglomerative (bottom-up) or divisive (top-down) approaches.\n",
    "\n",
    "Useful for creating dendrograms to visualize the data’s nested structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f9370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "linked = linkage(X, method='ward')\n",
    "dendrogram(linked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e8e49",
   "metadata": {},
   "source": [
    "### <a id=\"dbscan\"></a>DBScan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb68ea",
   "metadata": {},
   "source": [
    "Groups points that are closely packed together and marks points that lie alone in low-density regions as outliers.\n",
    "\n",
    "Effective for datasets with noise and varying density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bc4fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4473d0da",
   "metadata": {},
   "source": [
    "# III. Ensembling Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e845e8f",
   "metadata": {},
   "source": [
    "## <a id=\"bagging-random-forest\"></a>Bagging (Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca3f1e7",
   "metadata": {},
   "source": [
    "An extension of bagging that constructs multiple decision trees and merges them to get a more accurate and stable prediction.\n",
    "\n",
    "Handles both classification and regression tasks well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33102a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734b8744",
   "metadata": {},
   "source": [
    "## <a id=\"boosting\"></a>Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f4af0",
   "metadata": {},
   "source": [
    "Combines multiple models (usually of the same type) to reduce variance and improve stability.\n",
    "\n",
    "Useful for algorithms that exhibit high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a2d5f",
   "metadata": {},
   "source": [
    "### <a id=\"adaboost\"></a>AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe666d7c",
   "metadata": {},
   "source": [
    " A type of boosting that adjusts the weights of incorrectly classified instances, focusing more on difficult cases.\n",
    "\n",
    "Useful for improving the performance of weak classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7701af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(n_estimators=50)\n",
    "ada.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a9a5ee",
   "metadata": {},
   "source": [
    "### <a id=\"gradboost\"></a>GradBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7833830b",
   "metadata": {},
   "source": [
    "Sequentially builds models that minimize a loss function, typically using decision trees.\n",
    "\n",
    "Suitable for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d61898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=100)\n",
    "gbc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cacefd",
   "metadata": {},
   "source": [
    "### <a id=\"xgboost\"></a>XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff61fe4a",
   "metadata": {},
   "source": [
    "An optimized implementation of gradient boosting designed for speed and performance.\n",
    "\n",
    "Highly efficient and scalable for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384023a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100)\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56fd04b",
   "metadata": {},
   "source": [
    "# IV. Model Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d91f5",
   "metadata": {},
   "source": [
    "## <a id=\"confusion-matrix\"></a>Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e60877",
   "metadata": {},
   "source": [
    "A table that summarizes the performance of a classification model by showing the true vs. predicted classifications.\n",
    "\n",
    "Useful for evaluating the accuracy of a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b7b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382b949",
   "metadata": {},
   "source": [
    "## <a id=\"accuracy-metrics\"></a>Accuracy Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeb3c6d",
   "metadata": {},
   "source": [
    "Metrics used to evaluate the performance of a model, including accuracy, precision, recall, F1 score, etc.\n",
    "\n",
    "Provides different perspectives on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d3c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c8a33",
   "metadata": {},
   "source": [
    "## <a id=\"model-validation-techniques\"></a>Model Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd3b3c",
   "metadata": {},
   "source": [
    "### <a id=\"k-fold-cross-validation\"></a>k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5555e4aa",
   "metadata": {},
   "source": [
    "Splits the data into k subsets and trains the model k times, each time using a different subset as the validation set and the remaining as the training set.\n",
    "\n",
    "Provides a more reliable estimate of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e7e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "kf = KFold(n_splits=5)\n",
    "scores = cross_val_score(model, X, y, cv=kf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f58b4",
   "metadata": {},
   "source": [
    "### <a id=\"loocv\"></a>LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e19199",
   "metadata": {},
   "source": [
    "A special case of k-fold cross-validation where k equals the number of observations, i.e., each instance is used once as a validation set.\n",
    "\n",
    "Useful for small datasets to maximize training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "scores = cross_val_score(model, X, y, cv=loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f4c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f669cc9",
   "metadata": {},
   "source": [
    "# V. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803369dc",
   "metadata": {},
   "source": [
    "## <a id=\"Ridge\"></a>Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02313d5",
   "metadata": {},
   "source": [
    "Adds a penalty equal to the sum of the squared coefficients to the loss function, discouraging large coefficients.\n",
    "\n",
    "Reduces overfitting by constraining the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ed6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796443de",
   "metadata": {},
   "source": [
    "## <a id=\"Lasso\"></a>Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4947715b",
   "metadata": {},
   "source": [
    "Adds a penalty equal to the absolute value of the coefficients to the loss function, leading to sparse models.\n",
    "\n",
    "Useful for feature selection by shrinking some coefficients to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080067d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d512d",
   "metadata": {},
   "source": [
    "## <a id=\"ElasticNet\"></a>ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2e4b8f",
   "metadata": {},
   "source": [
    "Combines L1 and L2 regularization, adding both penalties to the loss function.\n",
    "\n",
    "Useful when there are multiple features with high collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "elastic_net.fit(X_train, y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
